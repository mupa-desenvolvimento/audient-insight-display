import { useEffect, useRef, useState, useCallback } from 'react';
import * as faceapi from 'face-api.js';
import { usePeopleRegistry } from './usePeopleRegistry';
import { useDetectionLog } from './useDetectionLog';
import { useAttentionHistory } from './useAttentionHistory';

// Face currently being tracked (looking at camera now)
export interface ActiveFace {
  trackId: string;
  personId?: string;
  name?: string;
  cpf?: string;
  gender: 'masculino' | 'feminino' | 'indefinido';
  ageGroup: '0-12' | '13-18' | '19-25' | '26-35' | '36-50' | '51+';
  age: number;
  ageEstimates: number[]; // Store multiple age readings for averaging
  confidence: number;
  position: { x: number; y: number; width: number; height: number };
  isRegistered: boolean;
  firstSeenAt: Date;
  lastSeenAt: Date;
  lookingDuration: number;
}

interface TrackedFaceData {
  descriptor: Float32Array;
  firstSeenAt: Date;
  lastSeenAt: Date;
  personId?: string;
  personName?: string;
  personCpf?: string;
  isRegistered: boolean;
  gender: 'masculino' | 'feminino' | 'indefinido';
  ageGroup: '0-12' | '13-18' | '19-25' | '26-35' | '36-50' | '51+';
  ageEstimates: number[];
  confidence: number;
  position: { x: number; y: number; width: number; height: number };
  loggedToHistory: boolean;
}

const getAgeGroup = (age: number): '0-12' | '13-18' | '19-25' | '26-35' | '36-50' | '51+' => {
  if (age <= 12) return '0-12';
  if (age <= 18) return '13-18';
  if (age <= 25) return '19-25';
  if (age <= 35) return '26-35';
  if (age <= 50) return '36-50';
  return '51+';
};

const getGender = (gender: string, genderProbability: number): 'masculino' | 'feminino' | 'indefinido' => {
  // Only classify if confidence is above 70%
  if (genderProbability < 0.7) return 'indefinido';
  if (gender === 'female') return 'feminino';
  if (gender === 'male') return 'masculino';
  return 'indefinido';
};

// Calculate average age from estimates, removing outliers
const calculateAverageAge = (estimates: number[]): number => {
  if (estimates.length === 0) return 0;
  if (estimates.length === 1) return Math.round(estimates[0]);
  
  // Sort and remove top/bottom 20% as outliers if we have enough samples
  const sorted = [...estimates].sort((a, b) => a - b);
  const trimCount = Math.floor(sorted.length * 0.2);
  const trimmed = sorted.slice(trimCount, sorted.length - trimCount);
  
  if (trimmed.length === 0) return Math.round(sorted[Math.floor(sorted.length / 2)]);
  
  const sum = trimmed.reduce((acc, val) => acc + val, 0);
  return Math.round(sum / trimmed.length);
};

// Check if face is looking at camera using landmarks
const isFacingCamera = (landmarks: faceapi.FaceLandmarks68): boolean => {
  const positions = landmarks.positions;
  
  // Get key facial points
  const leftEye = landmarks.getLeftEye();
  const rightEye = landmarks.getRightEye();
  const nose = landmarks.getNose();
  
  // Calculate eye centers
  const leftEyeCenter = {
    x: leftEye.reduce((sum, p) => sum + p.x, 0) / leftEye.length,
    y: leftEye.reduce((sum, p) => sum + p.y, 0) / leftEye.length
  };
  const rightEyeCenter = {
    x: rightEye.reduce((sum, p) => sum + p.x, 0) / rightEye.length,
    y: rightEye.reduce((sum, p) => sum + p.y, 0) / rightEye.length
  };
  
  // Get nose tip (index 30 in 68-point model)
  const noseTip = positions[30];
  
  // Calculate face width (distance between eyes)
  const eyeDistance = Math.abs(rightEyeCenter.x - leftEyeCenter.x);
  
  // Calculate center point between eyes
  const eyesCenterX = (leftEyeCenter.x + rightEyeCenter.x) / 2;
  
  // Calculate horizontal offset of nose from eyes center
  const noseOffset = Math.abs(noseTip.x - eyesCenterX);
  
  // Calculate ratio - if nose is too far from center, face is turned
  const turnRatio = noseOffset / eyeDistance;
  
  // Threshold: if nose offset is more than 25% of eye distance, face is turned away
  const horizontalThreshold = 0.25;
  
  // Also check vertical alignment - get jaw outline for face tilt
  const jawOutline = landmarks.getJawOutline();
  const jawLeft = jawOutline[0];
  const jawRight = jawOutline[16];
  
  // Calculate face tilt (if one side of jaw is much higher than other)
  const jawTilt = Math.abs(jawLeft.y - jawRight.y) / eyeDistance;
  const tiltThreshold = 0.4;
  
  const isFacingHorizontally = turnRatio < horizontalThreshold;
  const isNotTilted = jawTilt < tiltThreshold;
  
  return isFacingHorizontally && isNotTilted;
};

const FACE_MATCH_THRESHOLD = 0.45; // Stricter threshold for better matching
const FACE_TIMEOUT_MS = 2000; // Remove tracked face after 2 seconds of not being seen
const DETECTION_INTERVAL_MS = 500; // Faster detection for smoother tracking

export const useFaceDetection = (
  videoRef: React.RefObject<HTMLVideoElement>,
  canvasRef: React.RefObject<HTMLCanvasElement>,
  isActive: boolean
) => {
  const [isModelsLoaded, setIsModelsLoaded] = useState(false);
  const [activeFaces, setActiveFaces] = useState<ActiveFace[]>([]); // Currently looking
  const [isLoading, setIsLoading] = useState(false);
  const [totalSessionsToday, setTotalSessionsToday] = useState(0);
  
  const detectionIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const lastDetectedPersonsRef = useRef<Set<string>>(new Set());
  const trackedFacesRef = useRef<Map<string, TrackedFaceData>>(new Map());
  
  const { registeredPeople } = usePeopleRegistry();
  const { logDetection } = useDetectionLog();
  const { addAttentionRecord } = useAttentionHistory();

  // Load face-api.js models - use SSD MobileNet for better accuracy
  useEffect(() => {
    const loadModels = async () => {
      try {
        setIsLoading(true);
        
        const MODEL_URL = 'https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights';
        
        await Promise.all([
          faceapi.nets.ssdMobilenetv1.loadFromUri(MODEL_URL), // Better accuracy than TinyFace
          faceapi.nets.ageGenderNet.loadFromUri(MODEL_URL),
          faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
          faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL)
        ]);
        
        setIsModelsLoaded(true);
        console.log('Face-api.js models loaded successfully (SSD MobileNet)');
      } catch (error) {
        console.error('Error loading face-api.js models:', error);
      } finally {
        setIsLoading(false);
      }
    };

    loadModels();
  }, []);

  // Find matching tracked face by descriptor
  const findMatchingTrackedFace = useCallback((descriptor: Float32Array): string | null => {
    const trackedFaces = trackedFacesRef.current;
    let bestMatch: { trackId: string; distance: number } | null = null;
    
    for (const [trackId, tracked] of trackedFaces.entries()) {
      try {
        const distance = faceapi.euclideanDistance(descriptor, tracked.descriptor);
        if (distance < FACE_MATCH_THRESHOLD) {
          if (!bestMatch || distance < bestMatch.distance) {
            bestMatch = { trackId, distance };
          }
        }
      } catch (error) {
        console.error('Error comparing face descriptors:', error);
      }
    }
    return bestMatch?.trackId || null;
  }, []);

  // Update active faces state from tracked faces
  const updateActiveFacesState = useCallback(() => {
    const now = Date.now();
    const active: ActiveFace[] = [];
    
    trackedFacesRef.current.forEach((tracked, trackId) => {
      // Only include faces seen in the last FACE_TIMEOUT_MS
      if (now - tracked.lastSeenAt.getTime() <= FACE_TIMEOUT_MS) {
        const avgAge = calculateAverageAge(tracked.ageEstimates);
        const duration = (tracked.lastSeenAt.getTime() - tracked.firstSeenAt.getTime()) / 1000;
        
        active.push({
          trackId,
          personId: tracked.personId,
          name: tracked.personName,
          cpf: tracked.personCpf,
          gender: tracked.gender,
          ageGroup: getAgeGroup(avgAge),
          age: avgAge,
          ageEstimates: tracked.ageEstimates,
          confidence: tracked.confidence,
          position: tracked.position,
          isRegistered: tracked.isRegistered,
          firstSeenAt: tracked.firstSeenAt,
          lastSeenAt: tracked.lastSeenAt,
          lookingDuration: duration
        });
      }
    });
    
    setActiveFaces(active);
  }, []);

  // Clean up old tracked faces and save attention history
  useEffect(() => {
    const cleanupInterval = setInterval(() => {
      const now = Date.now();
      const trackedFaces = trackedFacesRef.current;
      
      for (const [trackId, tracked] of trackedFaces.entries()) {
        if (now - tracked.lastSeenAt.getTime() > FACE_TIMEOUT_MS) {
          const duration = (tracked.lastSeenAt.getTime() - tracked.firstSeenAt.getTime()) / 1000;
          const avgAge = calculateAverageAge(tracked.ageEstimates);
          
          console.log(`Face saiu do frame: ${tracked.personName || 'Desconhecido'} - duração: ${duration.toFixed(1)}s, idade média: ${avgAge}`);
          
          // Save to attention history only if duration >= 1 second
          if (duration >= 1) {
            addAttentionRecord(
              trackId,
              tracked.personId,
              tracked.personName,
              tracked.isRegistered,
              tracked.gender,
              tracked.ageGroup,
              avgAge,
              tracked.firstSeenAt,
              tracked.lastSeenAt,
              duration
            );
            setTotalSessionsToday(prev => prev + 1);
          }
          
          trackedFaces.delete(trackId);
        }
      }
      
      updateActiveFacesState();
    }, 500);

    return () => clearInterval(cleanupInterval);
  }, [addAttentionRecord, updateActiveFacesState]);

  const detectFaces = useCallback(async () => {
    if (!videoRef.current || !canvasRef.current || !isModelsLoaded || !isActive) {
      return;
    }

    const video = videoRef.current;
    const canvas = canvasRef.current;
    
    if (video.videoWidth === 0 || video.videoHeight === 0) {
      return;
    }

    try {
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      
      const ctx = canvas.getContext('2d');
      if (!ctx) return;

      ctx.clearRect(0, 0, canvas.width, canvas.height);

      // Use SSD MobileNet with higher confidence threshold
      const detections = await faceapi
        .detectAllFaces(video, new faceapi.SsdMobilenetv1Options({ minConfidence: 0.5 }))
        .withFaceLandmarks()
        .withFaceDescriptors()
        .withAgeAndGender();

      const now = new Date();
      const currentTrackIds = new Set<string>();

      for (let index = 0; index < detections.length; index++) {
        const detection = detections[index];
        
        // Skip faces not looking at camera
        if (!isFacingCamera(detection.landmarks)) {
          continue;
        }
        
        const box = detection.detection.box;
        const rawAge = detection.age;
        const genderProbability = detection.genderProbability;
        const genderString = detection.gender;
        const gender = getGender(genderString, genderProbability);
        const detectionConfidence = detection.detection.score;

        // Find or create tracked face
        let trackId: string | null = null;
        let existingTracked: TrackedFaceData | undefined;
        
        if (detection.descriptor) {
          trackId = findMatchingTrackedFace(detection.descriptor);
          
          if (trackId) {
            existingTracked = trackedFacesRef.current.get(trackId);
          }
        }

        // Identify registered person
        let identifiedPerson: { id: string; name: string; cpf: string; confidence: number } | null = null;
        if (detection.descriptor) {
          for (const person of registeredPeople) {
            try {
              // Use averageDescriptor for comparison with multiple captures
              const distance = faceapi.euclideanDistance(detection.descriptor, person.averageDescriptor);
              if (distance < 0.55) { // Stricter threshold
                identifiedPerson = {
                  id: person.id,
                  name: person.name,
                  cpf: person.cpf,
                  confidence: 1 - distance
                };
                break;
              }
            } catch (error) {
              console.error('Erro ao comparar face descriptor:', error);
            }
          }
        }

        const isRegistered = !!identifiedPerson;
        
        if (trackId && existingTracked) {
          // Update existing tracked face
          existingTracked.lastSeenAt = now;
          existingTracked.descriptor = detection.descriptor;
          existingTracked.ageEstimates.push(rawAge);
          // Keep only last 10 age estimates
          if (existingTracked.ageEstimates.length > 10) {
            existingTracked.ageEstimates.shift();
          }
          existingTracked.confidence = detectionConfidence;
          existingTracked.position = { x: box.x, y: box.y, width: box.width, height: box.height };
          
          // Update person info if now identified
          if (identifiedPerson && !existingTracked.personId) {
            existingTracked.personId = identifiedPerson.id;
            existingTracked.personName = identifiedPerson.name;
            existingTracked.personCpf = identifiedPerson.cpf;
            existingTracked.isRegistered = true;
          }
        } else {
          // Create new tracked face
          trackId = isRegistered ? identifiedPerson!.id : `track_${now.getTime()}_${index}`;
          
          trackedFacesRef.current.set(trackId, {
            descriptor: detection.descriptor,
            firstSeenAt: now,
            lastSeenAt: now,
            personId: identifiedPerson?.id,
            personName: identifiedPerson?.name,
            personCpf: identifiedPerson?.cpf,
            isRegistered,
            gender,
            ageGroup: getAgeGroup(rawAge),
            ageEstimates: [rawAge],
            confidence: detectionConfidence,
            position: { x: box.x, y: box.y, width: box.width, height: box.height },
            loggedToHistory: false
          });
        }

        currentTrackIds.add(trackId);
        
        // Log registered person detection (only once per session)
        if (isRegistered && !lastDetectedPersonsRef.current.has(identifiedPerson!.id)) {
          logDetection(
            identifiedPerson!.id,
            identifiedPerson!.name,
            identifiedPerson!.cpf,
            identifiedPerson!.confidence
          );
          
          lastDetectedPersonsRef.current.add(identifiedPerson!.id);
          setTimeout(() => {
            lastDetectedPersonsRef.current.delete(identifiedPerson!.id);
          }, 30000); // 30 seconds before allowing new log
        }

        // Get current data for drawing
        const tracked = trackedFacesRef.current.get(trackId);
        if (!tracked) continue;
        
        const avgAge = calculateAverageAge(tracked.ageEstimates);
        const duration = (tracked.lastSeenAt.getTime() - tracked.firstSeenAt.getTime()) / 1000;

        // Draw on canvas
        const color = tracked.isRegistered ? '#00ff00' : '#ff6600';
        
        ctx.strokeStyle = color;
        ctx.lineWidth = 3;
        ctx.strokeRect(box.x, box.y, box.width, box.height);
        
        // Main label
        ctx.font = 'bold 14px Arial';
        const label = tracked.isRegistered ? tracked.personName! : `${gender} | ${avgAge} anos`;
        const labelY = box.y > 50 ? box.y - 30 : box.y + box.height + 20;
        
        const textWidth = ctx.measureText(label).width;
        ctx.fillStyle = 'rgba(0, 0, 0, 0.8)';
        ctx.fillRect(box.x - 2, labelY - 14, textWidth + 8, 20);
        
        ctx.fillStyle = color;
        ctx.fillText(label, box.x + 2, labelY);
        
        // Duration label
        ctx.font = '12px Arial';
        const durationLabel = `⏱ ${duration.toFixed(1)}s`;
        const durationY = box.y > 50 ? box.y - 10 : box.y + box.height + 38;
        const durationWidth = ctx.measureText(durationLabel).width;
        
        ctx.fillStyle = 'rgba(0, 0, 0, 0.8)';
        ctx.fillRect(box.x - 2, durationY - 12, durationWidth + 8, 16);
        
        ctx.fillStyle = '#ffcc00';
        ctx.fillText(durationLabel, box.x + 2, durationY);
      }

      updateActiveFacesState();
      
    } catch (error) {
      console.error('Error during face detection:', error);
    }
  }, [videoRef, canvasRef, isModelsLoaded, isActive, registeredPeople, logDetection, findMatchingTrackedFace, updateActiveFacesState]);

  // Start/stop detection based on isActive
  useEffect(() => {
    if (isActive && isModelsLoaded) {
      detectionIntervalRef.current = setInterval(detectFaces, DETECTION_INTERVAL_MS);
    } else {
      if (detectionIntervalRef.current) {
        clearInterval(detectionIntervalRef.current);
        detectionIntervalRef.current = null;
      }
    }

    return () => {
      if (detectionIntervalRef.current) {
        clearInterval(detectionIntervalRef.current);
      }
    };
  }, [isActive, isModelsLoaded, detectFaces]);

  // Clear tracked faces when camera stops
  useEffect(() => {
    if (!isActive) {
      // Save all remaining tracked faces to history before clearing
      const now = new Date();
      trackedFacesRef.current.forEach((tracked, trackId) => {
        const duration = (tracked.lastSeenAt.getTime() - tracked.firstSeenAt.getTime()) / 1000;
        if (duration >= 1) {
          const avgAge = calculateAverageAge(tracked.ageEstimates);
          addAttentionRecord(
            trackId,
            tracked.personId,
            tracked.personName,
            tracked.isRegistered,
            tracked.gender,
            tracked.ageGroup,
            avgAge,
            tracked.firstSeenAt,
            tracked.lastSeenAt,
            duration
          );
        }
      });
      trackedFacesRef.current.clear();
      setActiveFaces([]);
    }
  }, [isActive, addAttentionRecord]);

  return {
    isModelsLoaded,
    isLoading,
    activeFaces, // Currently looking at camera
    totalLooking: activeFaces.length,
    totalSessionsToday
  };
};
